{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data in EMADE\n",
    "In this notebook I am going to cover the processing of cleaning and formatting a specific dataset for EMADE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import shutil\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing all the libraries we will need. The most important libraries are numpy and pandas because we will use their data types to store and manipulate our dataset.\n",
    "\n",
    "Note: Make sure you have nltk's corpus downloaded. You can check by running nltk.download() in a Python shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import our dataset into a pandas dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country                                        description  \\\n",
      "0      US  This tremendous 100% varietal wine hails from ...   \n",
      "1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
      "2      US  Mac Watson honors the memory of a wine once ma...   \n",
      "3      US  This spent 20 months in 30% new French oak, an...   \n",
      "4  France  This is the top wine from La Bégude, named aft...   \n",
      "\n",
      "                            designation  points  price        province  \\\n",
      "0                     Martha's Vineyard      96  235.0      California   \n",
      "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
      "2         Special Selected Late Harvest      96   90.0      California   \n",
      "3                               Reserve      96   65.0          Oregon   \n",
      "4                            La Brûlade      95   66.0        Provence   \n",
      "\n",
      "            region_1           region_2             variety  \\\n",
      "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
      "1               Toro                NaN       Tinta de Toro   \n",
      "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
      "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
      "4             Bandol                NaN  Provence red blend   \n",
      "\n",
      "                    winery  \n",
      "0                    Heitz  \n",
      "1  Bodega Carmen Rodríguez  \n",
      "2                 Macauley  \n",
      "3                    Ponzi  \n",
      "4     Domaine de la Bégude  \n",
      "(150930, 10)\n",
      "(25000, 10)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/winemag-data_first150k.csv\")\n",
    "\n",
    "# Print first 5 examples\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "# remove all rows except for the first 25,000\n",
    "data = data.drop(data.index[25000:])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original link to the dataset can be found here: https://www.kaggle.com/zynicide/wine-reviews\n",
    "\n",
    "However, I removed the index column from the csv in Excel.\n",
    "\n",
    "The dataset consists of 10 features. Most of these are string values, which machine learning algorithms and data transformations will not accept. The dataset also has columns with missing values. We will either have to remove those columns or replace the missing values.\n",
    "\n",
    "We also cut the dataset size down to 25,000 examples to reduce the size of our final dataset and RAM usage.\n",
    "\n",
    "Note: if your computer does not have at least 8 GB of RAM, you might need to reduce the size of the data even more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will separate a column for labels and modify its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "labels = data[[\"price\"]]\n",
    "print(labels.shape)\n",
    "\n",
    "# Fill in all missing values with mean of column\n",
    "labels = labels.fillna(labels.mean())\n",
    "\n",
    "# Set all values greater or equal to 50 to 1\n",
    "labels.loc[labels.price >= 50, 'price'] = 1\n",
    "labels.loc[labels.price != 1, 'price'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will choose to classify whether a wine has a price greater than or equal to 50. It is important to learn how to recognize potential labels and classification problems when you find a dataset. Most datasets will not have pre-defined labels and even those with labels you can tweak into a different problem.\n",
    "\n",
    "We also fill in the missing price values with the mean of all price values. There are many different methods of dealing with missing values. You could remove all missing value rows or replace missing values with the median instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will remove irrelevant and/or unuseful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"price\", axis=1)\n",
    "data = data.drop(\"region_2\", axis=1)\n",
    "data = data.drop(\"winery\", axis=1)\n",
    "data = data.drop(\"designation\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed these features for a specific reason. Price needs to be removed because we will add it back as labels later. Region 2 is similar enough to region_1 to not add useful information. Winery and designation have too many unique values.\n",
    "\n",
    "When we one-hot encode some of our features in the next section, too many unique values will matter. If most of the strings in a feature are unique, we get very little useful or relevant information and only make the dimensions of our data unnecessarily large. This also applies to region 2. We do not want to double our dimensions with overlapping information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will one-hot encoding on our remaining string features to extract relevant information from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=[\"country\", \"province\", \"variety\", \"region_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of one-hot encoding can be found here: https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will separate the descriptions column and clean up the text data. One-hot encoding does not work well on columns with unique sentences or paragraphs because all of the items in the column will end up being unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tremendous 100% varietal wine hails from Oakville and was aged over three years in oak. Juicy red-cherry fruit and a compelling hint of caramel greet the palate, framed by elegant, fine tannins and a subtle minty tone in the background. Balanced and rewarding from start to finish, it has years ahead of it to develop further nuance. Enjoy 2022–2030.\n",
      "line 1 processed\n",
      "line 2 processed\n",
      "line 3 processed\n",
      "line 4 processed\n",
      "line 5 processed\n",
      "line 6 processed\n",
      "line 7 processed\n",
      "line 8 processed\n",
      "line 9 processed\n",
      "line 10 processed\n",
      "line 11 processed\n",
      "line 12 processed\n",
      "line 13 processed\n",
      "line 14 processed\n",
      "line 15 processed\n",
      "line 16 processed\n",
      "line 17 processed\n",
      "line 18 processed\n",
      "line 19 processed\n",
      "line 20 processed\n",
      "line 21 processed\n",
      "line 22 processed\n",
      "line 23 processed\n",
      "line 24 processed\n",
      "line 25 processed\n",
      "line 26 processed\n",
      "line 27 processed\n",
      "line 28 processed\n",
      "line 29 processed\n",
      "line 30 processed\n",
      "line 31 processed\n",
      "line 32 processed\n",
      "line 33 processed\n",
      "line 34 processed\n",
      "line 35 processed\n",
      "line 36 processed\n",
      "line 37 processed\n",
      "line 38 processed\n",
      "line 39 processed\n",
      "line 40 processed\n",
      "line 41 processed\n",
      "line 42 processed\n",
      "line 43 processed\n",
      "line 44 processed\n",
      "line 45 processed\n",
      "line 46 processed\n",
      "line 47 processed\n",
      "line 48 processed\n",
      "line 49 processed\n",
      "line 50 processed\n",
      "line 51 processed\n",
      "line 52 processed\n",
      "line 53 processed\n",
      "line 54 processed\n",
      "line 55 processed\n",
      "line 56 processed\n",
      "line 57 processed\n",
      "line 58 processed\n",
      "line 59 processed\n",
      "line 60 processed\n",
      "line 61 processed\n",
      "line 62 processed\n",
      "line 63 processed\n",
      "line 64 processed\n",
      "line 65 processed\n",
      "line 66 processed\n",
      "line 67 processed\n",
      "line 68 processed\n",
      "line 69 processed\n",
      "line 70 processed\n",
      "line 71 processed\n",
      "line 72 processed\n",
      "line 73 processed\n",
      "line 74 processed\n",
      "line 75 processed\n",
      "line 76 processed\n",
      "line 77 processed\n",
      "line 78 processed\n",
      "line 79 processed\n",
      "line 80 processed\n",
      "line 81 processed\n",
      "line 82 processed\n",
      "line 83 processed\n",
      "line 84 processed\n",
      "line 85 processed\n",
      "line 86 processed\n",
      "line 87 processed\n",
      "line 88 processed\n",
      "line 89 processed\n",
      "line 90 processed\n",
      "line 91 processed\n",
      "line 92 processed\n",
      "line 93 processed\n",
      "line 94 processed\n",
      "line 95 processed\n",
      "line 96 processed\n",
      "line 97 processed\n",
      "line 98 processed\n",
      "line 99 processed\n",
      "line 100 processed\n",
      "line 101 processed\n",
      "line 102 processed\n",
      "line 103 processed\n",
      "line 104 processed\n",
      "line 105 processed\n",
      "line 106 processed\n",
      "line 107 processed\n",
      "line 108 processed\n",
      "line 109 processed\n",
      "line 110 processed\n",
      "line 111 processed\n",
      "line 112 processed\n",
      "line 113 processed\n",
      "line 114 processed\n",
      "line 115 processed\n",
      "line 116 processed\n",
      "line 117 processed\n",
      "line 118 processed\n",
      "line 119 processed\n",
      "line 120 processed\n",
      "line 121 processed\n",
      "line 122 processed\n",
      "line 123 processed\n",
      "line 124 processed\n",
      "line 125 processed\n",
      "line 126 processed\n",
      "line 127 processed\n",
      "line 128 processed\n",
      "line 129 processed\n",
      "line 130 processed\n",
      "line 131 processed\n",
      "line 132 processed\n",
      "line 133 processed\n",
      "line 134 processed\n",
      "line 135 processed\n",
      "line 136 processed\n",
      "line 137 processed\n",
      "line 138 processed\n",
      "line 139 processed\n",
      "line 140 processed\n",
      "line 141 processed\n",
      "line 142 processed\n",
      "line 143 processed\n",
      "line 144 processed\n",
      "line 145 processed\n",
      "line 146 processed\n",
      "line 147 processed\n",
      "line 148 processed\n",
      "line 149 processed\n",
      "line 150 processed\n",
      "line 151 processed\n",
      "line 152 processed\n",
      "line 153 processed\n",
      "line 154 processed\n",
      "line 155 processed\n",
      "line 156 processed\n",
      "line 157 processed\n",
      "line 158 processed\n",
      "line 159 processed\n",
      "line 160 processed\n",
      "line 161 processed\n",
      "line 162 processed\n",
      "line 163 processed\n",
      "line 164 processed\n",
      "line 165 processed\n",
      "line 166 processed\n",
      "line 167 processed\n",
      "line 168 processed\n",
      "line 169 processed\n",
      "line 170 processed\n",
      "line 171 processed\n",
      "line 172 processed\n",
      "line 173 processed\n",
      "line 174 processed\n",
      "line 175 processed\n",
      "line 176 processed\n",
      "line 177 processed\n",
      "line 178 processed\n",
      "line 179 processed\n",
      "line 180 processed\n",
      "line 181 processed\n",
      "line 182 processed\n",
      "line 183 processed\n",
      "line 184 processed\n",
      "line 185 processed\n",
      "line 186 processed\n",
      "line 187 processed\n",
      "line 188 processed\n",
      "line 189 processed\n",
      "line 190 processed\n",
      "line 191 processed\n",
      "line 192 processed\n",
      "line 193 processed\n",
      "line 194 processed\n",
      "line 195 processed\n",
      "line 196 processed\n",
      "line 197 processed\n",
      "line 198 processed\n",
      "line 199 processed\n",
      "line 200 processed\n",
      "line 201 processed\n",
      "line 202 processed\n",
      "line 203 processed\n",
      "line 204 processed\n",
      "line 205 processed\n",
      "line 206 processed\n",
      "line 207 processed\n",
      "line 208 processed\n",
      "line 209 processed\n",
      "line 210 processed\n",
      "line 211 processed\n",
      "line 212 processed\n",
      "line 213 processed\n",
      "line 214 processed\n",
      "line 215 processed\n",
      "line 216 processed\n",
      "line 217 processed\n",
      "line 218 processed\n",
      "line 219 processed\n",
      "line 220 processed\n",
      "line 221 processed\n",
      "line 222 processed\n",
      "line 223 processed\n",
      "line 224 processed\n",
      "line 225 processed\n",
      "line 226 processed\n",
      "line 227 processed\n",
      "line 228 processed\n",
      "line 229 processed\n",
      "line 230 processed\n",
      "line 231 processed\n",
      "line 232 processed\n",
      "line 233 processed\n",
      "line 234 processed\n",
      "line 235 processed\n",
      "line 236 processed\n",
      "line 237 processed\n",
      "line 238 processed\n",
      "line 239 processed\n",
      "line 240 processed\n",
      "line 241 processed\n",
      "line 242 processed\n",
      "line 243 processed\n",
      "line 244 processed\n",
      "line 245 processed\n",
      "line 246 processed\n",
      "line 247 processed\n",
      "line 248 processed\n",
      "line 249 processed\n",
      "line 250 processed\n",
      "line 251 processed\n",
      "line 252 processed\n",
      "line 253 processed\n",
      "line 254 processed\n",
      "line 255 processed\n",
      "line 256 processed\n",
      "line 257 processed\n",
      "line 258 processed\n",
      "line 259 processed\n",
      "line 260 processed\n",
      "line 261 processed\n",
      "line 262 processed\n",
      "line 263 processed\n",
      "line 264 processed\n",
      "line 265 processed\n",
      "line 266 processed\n",
      "line 267 processed\n",
      "line 268 processed\n",
      "line 269 processed\n",
      "line 270 processed\n",
      "line 271 processed\n",
      "line 272 processed\n",
      "line 273 processed\n",
      "line 274 processed\n",
      "line 275 processed\n",
      "line 276 processed\n",
      "line 277 processed\n",
      "line 278 processed\n",
      "line 279 processed\n",
      "line 280 processed\n",
      "line 281 processed\n",
      "line 282 processed\n",
      "line 283 processed\n",
      "line 284 processed\n",
      "line 285 processed\n",
      "line 286 processed\n",
      "line 287 processed\n",
      "line 288 processed\n",
      "line 289 processed\n",
      "line 290 processed\n",
      "line 291 processed\n",
      "line 292 processed\n",
      "line 293 processed\n",
      "line 294 processed\n",
      "line 295 processed\n",
      "line 296 processed\n",
      "line 297 processed\n",
      "line 298 processed\n",
      "line 299 processed\n",
      "line 300 processed\n",
      "line 301 processed\n",
      "line 302 processed\n",
      "line 303 processed\n",
      "line 304 processed\n",
      "line 305 processed\n",
      "line 306 processed\n",
      "line 307 processed\n",
      "line 308 processed\n",
      "line 309 processed\n",
      "line 310 processed\n",
      "line 311 processed\n",
      "line 312 processed\n",
      "line 313 processed\n",
      "line 314 processed\n",
      "line 315 processed\n",
      "line 316 processed\n",
      "line 317 processed\n",
      "line 318 processed\n",
      "line 319 processed\n",
      "line 320 processed\n",
      "line 321 processed\n",
      "line 322 processed\n",
      "line 323 processed\n",
      "line 324 processed\n",
      "line 325 processed\n",
      "line 326 processed\n",
      "line 327 processed\n",
      "line 328 processed\n",
      "line 329 processed\n",
      "line 330 processed\n",
      "line 331 processed\n",
      "line 332 processed\n",
      "line 333 processed\n",
      "line 334 processed\n",
      "line 335 processed\n",
      "line 336 processed\n",
      "line 337 processed\n",
      "line 338 processed\n",
      "line 339 processed\n",
      "line 340 processed\n",
      "line 341 processed\n",
      "line 342 processed\n",
      "line 343 processed\n",
      "line 344 processed\n",
      "line 345 processed\n",
      "line 346 processed\n",
      "line 347 processed\n",
      "line 348 processed\n",
      "line 349 processed\n",
      "line 350 processed\n",
      "line 351 processed\n",
      "line 352 processed\n",
      "line 353 processed\n",
      "line 354 processed\n",
      "line 355 processed\n",
      "line 356 processed\n",
      "line 357 processed\n",
      "line 358 processed\n",
      "line 359 processed\n",
      "line 360 processed\n",
      "line 361 processed\n",
      "line 362 processed\n",
      "line 363 processed\n",
      "line 364 processed\n",
      "line 365 processed\n",
      "line 366 processed\n",
      "line 367 processed\n",
      "line 368 processed\n",
      "line 369 processed\n",
      "line 370 processed\n",
      "line 371 processed\n",
      "line 372 processed\n",
      "line 373 processed\n",
      "line 374 processed\n",
      "line 375 processed\n",
      "line 376 processed\n",
      "line 377 processed\n",
      "line 378 processed\n",
      "line 379 processed\n",
      "line 380 processed\n",
      "line 381 processed\n",
      "line 382 processed\n",
      "line 383 processed\n",
      "line 384 processed\n",
      "line 385 processed\n",
      "line 386 processed\n",
      "line 387 processed\n",
      "line 388 processed\n",
      "line 389 processed\n",
      "line 390 processed\n",
      "line 391 processed\n",
      "line 392 processed\n",
      "line 393 processed\n",
      "line 394 processed\n",
      "line 395 processed\n",
      "line 396 processed\n",
      "line 397 processed\n",
      "line 398 processed\n",
      "line 399 processed\n",
      "line 400 processed\n",
      "line 401 processed\n",
      "line 402 processed\n",
      "line 403 processed\n",
      "line 404 processed\n",
      "line 405 processed\n",
      "line 406 processed\n",
      "line 407 processed\n",
      "line 408 processed\n",
      "line 409 processed\n",
      "line 410 processed\n",
      "line 411 processed\n",
      "line 412 processed\n",
      "line 413 processed\n"
     ]
    }
   ],
   "source": [
    "text = data[[\"description\"]].values\n",
    "data = data.drop(\"description\", axis=1)\n",
    "\n",
    "text_list = []\n",
    "for i in text:\n",
    "    text_list.append(i[0])\n",
    "    \n",
    "# Alternative\n",
    "# text = text.tolist()\n",
    "# text = [i[0] for i in text]\n",
    "    \n",
    "# For debugging\n",
    "print(text_list[0])\n",
    "count = 0\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for text in text_list:\n",
    "    count += 1\n",
    "    text = text.lower()\n",
    "    text = re.sub('[!@#$.,?]', '', text)\n",
    "    words = text.split(\" \")\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    text = \" \".join(words)\n",
    "    print(\"line \" + str(count) + \" processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we separate the text column into its own numpy array. Then, we convert the numpy array into a list. This puts our data into a universal list format for when we clean the data.\n",
    "\n",
    "Next, we initialize variables we need and start looping over the individual text descriptions.\n",
    "\n",
    "We start by using regular expressions to get rid of punctuation and symbols, and we also make all of the text lower case. This makes sure repeated words are recognized in our text.\n",
    "\n",
    "Next, we split our description string into words by splitting on the spaces between words. This allows us to modify individual words in our text.\n",
    "\n",
    "Then, we remove stop words from our text and stem all our words. Stop words are words such as \"and\", \"the\", and \"of\" which do not have a lot of sentimental value to our machine learning. You can think of it as removing noise. However, you have to be careful. Removing stop words on a dataset with short phrases such as \"On the blue river\" and \"Over the mountain\" can cause a loss of important information.\n",
    "\n",
    "When we stem all the words we are removing irrelevant part of words such as prefixes and suffixes which are similar. We use the Porter Stemmer, which is well known to be a good solution to stemming words. Stemming also tends to reduce the amount of words in our bag of words model later on, which can help lower the dimensionality of our input data. However, we need to be careful to not stem useful information like with removing stop words.\n",
    "\n",
    "Finally, we join the modified words back together with spaces and keep track of our progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will vectorize our text descriptions and add our text features back into our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = np.array(text_list)\n",
    "data_array = data.values\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "text_vect = tfidf.fit_transform(text_array)\n",
    "\n",
    "print(data_array.shape)\n",
    "print(text_vect.shape)\n",
    "\n",
    "full_data = np.concatenate((text_vect.toarray(), data_array), axis=1)\n",
    "\n",
    "print(full_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert the main dataframe into a numpy array and the modified text list we made into a numpy array. Then, we use sklearn's term frequency vectorizer to vectorize our text data. Rather than simply counting the words in each description, we weight words based on term frequency. This vectorizer keeps less useful repeated words from overshadowing more useful unique words.\n",
    "\n",
    "Then, we append our vectorized text data onto our input data and observe the new shape of the data. The feature count may look very large compared to the number of examples. However, this is due to us having to cut down the number of examples from 150,000 to 25,000. The final feature count with 150,000 examples is only around 31,000, which suggests a good amount of repeated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about tf-idf term weighting here: http://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split our data into train and test data concat the input data and labels together to fit EMADE's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_data, labels.values, test_size=0.33)\n",
    "\n",
    "data_train = np.concatenate((X_train, y_train), axis=1)\n",
    "data_test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and test data with 67% as training and 33% as testing data. Then, we append the labels columns to the end of the input data columns to put the data into the format EMADE expects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will test out the performance of our dataset on a logistic regression classifier to get a rough benchmark. Logistic Regression works well on text datasets and any datasets with a lot of discrete and/or Boolean (1,0) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(y_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will convert our train and test data back into pandas dataframes and export it in the proper format for EMADE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(data_train)\n",
    "test = pd.DataFrame(data_test)\n",
    "\n",
    "divisor_train = math.ceil(len(train) / 5)\n",
    "divisor_test = math.ceil(len(test) / 5)\n",
    "\n",
    "count = 0\n",
    "for g, df in train.groupby(np.arange(len(train)) // divisor_train):\n",
    "    print(df.shape)\n",
    "\n",
    "    np.savetxt(\"datasets/wine_data_set_train_%i.txt\" % count, df.values, fmt='%.5f', delimiter=\",\")\n",
    "\n",
    "    with open(\"datasets/wine_data_set_train_%i.txt\" % count, \"rb\") as f_in, gzip.open(\"datasets/wine_data_set_train_%i.dat.gz\" % count, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "    os.remove(\"datasets/wine_data_set_train_%i.txt\" % count)\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "count = 0\n",
    "for g, df in test.groupby(np.arange(len(test)) // divisor_test):\n",
    "    print(df.shape)\n",
    "\n",
    "    np.savetxt(\"datasets/wine_data_set_test_%i.txt\" % count, df.values, fmt='%.5f', delimiter=\",\")\n",
    "\n",
    "    with open(\"datasets/wine_data_set_test_%i.txt\" % count, \"rb\") as f_in, gzip.open(\"datasets/wine_data_set_test_%i.dat.gz\" % count, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "    os.remove(\"datasets/wine_data_set_test_%i.txt\" % count)\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "small_train = train.drop(train.index[1675:])\n",
    "small_test = test.drop(test.index[825:])\n",
    "\n",
    "np.savetxt(\"datasets/wine_data_set_train_small.txt\", small_train.values, fmt='%.5f', delimiter=\",\")\n",
    "\n",
    "with open(\"datasets/wine_data_set_train_small.txt\", \"rb\") as f_in, gzip.open(\"datasets/wine_data_set_train_small.dat.gz\", \"wb\") as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "os.remove(\"datasets/wine_data_set_train_small.txt\")\n",
    "              \n",
    "np.savetxt(\"datasets/wine_data_set_test_small.txt\", small_train.values, fmt='%.5f', delimiter=\",\")\n",
    "\n",
    "with open(\"datasets/wine_data_set_test_small.txt\", \"rb\") as f_in, gzip.open(\"datasets/wine_data_set_test_small.dat.gz\", \"wb\") as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "os.remove(\"datasets/wine_data_set_test_small.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into 5 chunks each with 20% of the data. Then, we set aside one chunk of 10% to be our small dataset in EMADE.\n",
    "\n",
    "The final step for preprocessing is to compress all the .dat files into .dat.gz files and place them in a folder under the datasets directory of EMADE.\n",
    "\n",
    "There are other datasets already implemented into EMADE in this format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example xml template for the dataset we preprocessed. This template is used to setup parameters for EMADE. You can change the objectives, crossover probability, and mutation probability from here.\n",
    "\n",
    "Make sure to make an xml file and store it in the templates directory when you implement a new dataset into EMADE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\"?>\n",
    "\n",
    "<input>\n",
    "\n",
    "    <datasets>\n",
    "        <dataset>\n",
    "            <name>SmallDataSet</name>\n",
    "            <type>featuredata</type>\n",
    "            <MonteCarlo>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_small.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_small.dat.gz</testFilename>\n",
    "                </trial>\n",
    "            </MonteCarlo>\n",
    "        </dataset>\n",
    "        <dataset>\n",
    "            <name>FullDataSet</name>\n",
    "            <type>featuredata</type>\n",
    "            <MonteCarlo>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_0.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_0.dat.gz</testFilename>\n",
    "                </trial>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_1.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_1.dat.gz</testFilename>\n",
    "                </trial>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_2.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_2.dat.gz</testFilename>\n",
    "                </trial>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_3.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_3.dat.gz</testFilename>\n",
    "                </trial>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_4.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_4.dat.gz</testFilename>\n",
    "                </trial>\n",
    "            </MonteCarlo>\n",
    "        </dataset>\n",
    "    </datasets>\n",
    "\n",
    "    <objectives>\n",
    "        <objective>\n",
    "            <name>False Positives</name>\n",
    "            <weight>-1.0</weight>\n",
    "            <achievable>4971.8</achievable>\n",
    "            <goal>0</goal>\n",
    "            <evaluationFunction>false_positive</evaluationFunction>\n",
    "            <lower>0</lower>\n",
    "            <upper>1</upper>\n",
    "        </objective>\n",
    "        <objective>\n",
    "            <name>False Negatives</name>\n",
    "            <weight>-1.0</weight>\n",
    "            <achievable>1541.2</achievable>\n",
    "            <goal>0</goal>\n",
    "            <evaluationFunction>false_negative</evaluationFunction>\n",
    "            <lower>0</lower>\n",
    "            <upper>1</upper>\n",
    "        </objective>\n",
    "        <objective>\n",
    "            <name>F1-Score</name>\n",
    "            <weight>-1.0</weight>\n",
    "            <achievable>0.2</achievable>\n",
    "            <goal>0</goal>\n",
    "            <evaluationFunction>f1_score_min</evaluationFunction>\n",
    "            <lower>0</lower>\n",
    "            <upper>1</upper>\n",
    "        </objective>\n",
    "        <objective>\n",
    "            <name>Num Elements</name>\n",
    "            <weight>-1.0</weight>\n",
    "            <achievable>100.0</achievable>\n",
    "            <goal>0</goal>\n",
    "            <evaluationFunction>num_elements_eval_function</evaluationFunction>\n",
    "            <lower>0</lower>\n",
    "            <upper>1</upper>\n",
    "        </objective>\n",
    "\n",
    "    </objectives>\n",
    "\n",
    "    <evaluation>\n",
    "        <module>evalFunctions</module>\n",
    "        <memoryLimit>30</memoryLimit> <!-- In Percent -->\n",
    "    </evaluation>\n",
    "\n",
    "    <scoopParameters>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>24</workers>\n",
    "        </host>\n",
    "        <!--<host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>-->\n",
    "    </scoopParameters>\n",
    "\n",
    "    <evolutionParameters>\n",
    "        <initialPopulationSize>512</initialPopulationSize>\n",
    "        <elitePoolSize>512</elitePoolSize>\n",
    "        <launchSize>300</launchSize>\n",
    "        <minQueueSize>200</minQueueSize>\n",
    "\n",
    "        <matings>\n",
    "            <mating>\n",
    "                <name>crossover</name>\n",
    "                <probability>0.50</probability>\n",
    "            </mating>\n",
    "            <mating>\n",
    "                <name>crossoverEphemeral</name>\n",
    "                <probability>0.50</probability>\n",
    "            </mating>\n",
    "            <mating>\n",
    "                <name>headlessChicken</name>\n",
    "                <probability>0.10</probability>\n",
    "            </mating>\n",
    "            <mating>\n",
    "                <name>headlessChickenEphemeral</name>\n",
    "                <probability>0.10</probability>\n",
    "            </mating>\n",
    "        </matings>\n",
    "\n",
    "        <mutations>\n",
    "            <mutation>\n",
    "                <name>insert</name>\n",
    "                <probability>0.05</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>insert modify</name>\n",
    "                <probability>0.10</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>ephemeral</name>\n",
    "                <probability>0.25</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>node replace</name>\n",
    "                <probability>0.05</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>uniform</name>\n",
    "                <probability>0.05</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>shrink</name>\n",
    "                <probability>0.05</probability>\n",
    "            </mutation>\n",
    "        </mutations>\n",
    "\n",
    "        <selections>\n",
    "            <selection>\n",
    "                <name>NSGA2</name>\n",
    "            </selection>\n",
    "        </selections>\n",
    "\n",
    "    </evolutionParameters>\n",
    "\n",
    "    <seedFile>\n",
    "\n",
    "    </seedFile>\n",
    "\n",
    "    <genePoolFitness>\n",
    "        <prefix>genePoolFitnessWine</prefix>\n",
    "    </genePoolFitness>\n",
    "    <paretoFitness>\n",
    "        <prefix>paretoFitnessWine</prefix>\n",
    "    </paretoFitness>\n",
    "    <parentsOutput>\n",
    "        <prefix>parentsWine</prefix>\n",
    "    </parentsOutput>\n",
    "\n",
    "\n",
    "\n",
    "    <paretoOutput>\n",
    "        <prefix>paretoFrontWine</prefix>\n",
    "    </paretoOutput>\n",
    "</input>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
