{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data in EMADE\n",
    "In this notebook I am going to cover the processing of cleaning and formatting a specific dataset for EMADE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import shutil\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing all the libraries we will need. The most important libraries are numpy and pandas because we will use their data types to store and manipulate our dataset.\n",
    "\n",
    "Note: Make sure you have nltk's corpus downloaded. You can check by running nltk.download() in a Python shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import our dataset into a pandas dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country                                        description  \\\n",
      "0      US  This tremendous 100% varietal wine hails from ...   \n",
      "1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
      "2      US  Mac Watson honors the memory of a wine once ma...   \n",
      "3      US  This spent 20 months in 30% new French oak, an...   \n",
      "4  France  This is the top wine from La Bégude, named aft...   \n",
      "\n",
      "                            designation  points  price        province  \\\n",
      "0                     Martha's Vineyard      96  235.0      California   \n",
      "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
      "2         Special Selected Late Harvest      96   90.0      California   \n",
      "3                               Reserve      96   65.0          Oregon   \n",
      "4                            La Brûlade      95   66.0        Provence   \n",
      "\n",
      "            region_1           region_2             variety  \\\n",
      "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
      "1               Toro                NaN       Tinta de Toro   \n",
      "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
      "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
      "4             Bandol                NaN  Provence red blend   \n",
      "\n",
      "                    winery  \n",
      "0                    Heitz  \n",
      "1  Bodega Carmen Rodríguez  \n",
      "2                 Macauley  \n",
      "3                    Ponzi  \n",
      "4     Domaine de la Bégude  \n",
      "(150930, 10)\n",
      "(25000, 10)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/winemag-data_first150k.csv\")\n",
    "\n",
    "# Print first 5 examples\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "# remove all rows except for the first 25,000\n",
    "data = data.drop(data.index[25000:])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original link to the dataset can be found here: https://www.kaggle.com/zynicide/wine-reviews\n",
    "\n",
    "However, I removed the index column from the csv in Excel.\n",
    "\n",
    "The dataset consists of 10 features. Most of these are string values, which machine learning algorithms and data transformations will not accept. The dataset also has columns with missing values. We will either have to remove those columns or replace the missing values.\n",
    "\n",
    "We also cut the dataset size down to 25,000 examples to reduce the size of our final dataset and RAM usage.\n",
    "\n",
    "Note: if your computer does not have at least 8 GB of RAM, you might need to reduce the size of the data even more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will separate a column for labels and modify its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "labels = data[[\"price\"]]\n",
    "print(labels.shape)\n",
    "\n",
    "# Fill in all missing values with mean of column\n",
    "labels = labels.fillna(labels.mean())\n",
    "\n",
    "# Set all values greater or equal to 50 to 1\n",
    "labels.loc[labels.price >= 50, 'price'] = 1\n",
    "labels.loc[labels.price != 1, 'price'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will choose to classify whether a wine has a price greater than or equal to 50. It is important to learn how to recognize potential labels and classification problems when you find a dataset. Most datasets will not have pre-defined labels and even those with labels you can tweak into a different problem.\n",
    "\n",
    "We also fill in the missing price values with the mean of all price values. There are many different methods of dealing with missing values. You could remove all missing value rows or replace missing values with the median instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will remove irrelevant and/or unuseful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"price\", axis=1)\n",
    "data = data.drop(\"region_2\", axis=1)\n",
    "data = data.drop(\"winery\", axis=1)\n",
    "data = data.drop(\"designation\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed these features for a specific reason. Price needs to be removed because we will add it back as labels later. Region 2 is similar enough to region_1 to not add useful information. Winery and designation have too many unique values.\n",
    "\n",
    "When we one-hot encode some of our features in the next section, too many unique values will matter. If most of the strings in a feature are unique, we get very little useful or relevant information and only make the dimensions of our data unnecessarily large. This also applies to region 2. We do not want to double our dimensions with overlapping information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will one-hot encoding on our remaining string features to extract relevant information from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=[\"country\", \"province\", \"variety\", \"region_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of one-hot encoding can be found here: https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will separate the descriptions column and clean up the text data. One-hot encoding does not work well on columns with unique sentences or paragraphs because all of the items in the column will end up being unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tremendous 100% varietal wine hails from Oakville and was aged over three years in oak. Juicy red-cherry fruit and a compelling hint of caramel greet the palate, framed by elegant, fine tannins and a subtle minty tone in the background. Balanced and rewarding from start to finish, it has years ahead of it to develop further nuance. Enjoy 2022–2030.\n"
     ]
    }
   ],
   "source": [
    "text = data[[\"description\"]].values\n",
    "data = data.drop(\"description\", axis=1)\n",
    "\n",
    "text_list = []\n",
    "for i in text:\n",
    "    text_list.append(i[0])\n",
    "    \n",
    "# Alternative\n",
    "# text = text.tolist()\n",
    "# text = [i[0] for i in text]\n",
    "    \n",
    "# For debugging\n",
    "print(text_list[0])\n",
    "count = 0\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for text in text_list:\n",
    "    count += 1\n",
    "    text = text.lower()\n",
    "    text = re.sub('[!@#$.,?]', '', text)\n",
    "    words = text.split(\" \")\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    text = \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we separate the text column into its own numpy array. Then, we convert the numpy array into a list. This puts our data into a universal list format for when we clean the data.\n",
    "\n",
    "Next, we initialize variables we need and start looping over the individual text descriptions.\n",
    "\n",
    "We start by using regular expressions to get rid of punctuation and symbols, and we also make all of the text lower case. This makes sure repeated words are recognized in our text.\n",
    "\n",
    "Next, we split our description string into words by splitting on the spaces between words. This allows us to modify individual words in our text.\n",
    "\n",
    "Then, we remove stop words from our text and stem all our words. Stop words are words such as \"and\", \"the\", and \"of\" which do not have a lot of sentimental value to our machine learning. You can think of it as removing noise. However, you have to be careful. Removing stop words on a dataset with short phrases such as \"On the blue river\" and \"Over the mountain\" can cause a loss of important information.\n",
    "\n",
    "When we stem all the words we are removing irrelevant part of words such as prefixes and suffixes which are similar. We use the Porter Stemmer, which is well known to be a good solution to stemming words. Stemming also tends to reduce the amount of words in our bag of words model later on, which can help lower the dimensionality of our input data. However, we need to be careful to not stem useful information like with removing stop words.\n",
    "\n",
    "Finally, we join the modified words back together with spaces and keep track of our progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will vectorize our text descriptions and add our text features back into our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = np.array(text_list)\n",
    "data_array = data.values\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "text_vect = tfidf.fit_transform(text_array)\n",
    "\n",
    "print(data_array.shape)\n",
    "print(text_vect.shape)\n",
    "\n",
    "full_data = np.concatenate((text_vect.toarray(), data_array), axis=1)\n",
    "\n",
    "print(full_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert the main dataframe into a numpy array and the modified text list we made into a numpy array. Then, we use sklearn's term frequency vectorizer to vectorize our text data. Rather than simply counting the words in each description, we weight words based on term frequency. This vectorizer keeps less useful repeated words from overshadowing more useful unique words.\n",
    "\n",
    "Then, we append our vectorized text data onto our input data and observe the new shape of the data. The feature count may look very large compared to the number of examples. However, this is due to us having to cut down the number of examples from 150,000 to 25,000. The final feature count with 150,000 examples is only around 31,000, which suggests a good amount of repeated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about tf-idf term weighting here: http://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split our data into train and test data concat the input data and labels together to fit EMADE's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_data, labels.values, test_size=0.33)\n",
    "\n",
    "data_train = np.concatenate((X_train, y_train), axis=1)\n",
    "data_test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and test data with 67% as training and 33% as testing data. Then, we append the labels columns to the end of the input data columns to put the data into the format EMADE expects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will test out the performance of our dataset on a logistic regression classifier to get a rough benchmark. Logistic Regression works well on text datasets and any datasets with a lot of discrete and/or Boolean (1,0) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(y_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will convert our train and test data back into pandas dataframes and export it in the proper format for EMADE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(data_train)\n",
    "test = pd.DataFrame(data_test)\n",
    "\n",
    "divisor_train = math.ceil(len(train) / 5)\n",
    "divisor_test = math.ceil(len(test) / 5)\n",
    "\n",
    "count = 0\n",
    "for g, df in train.groupby(np.arange(len(train)) // divisor_train):\n",
    "    print(df.shape)\n",
    "\n",
    "    np.savetxt(\"datasets/wine_data_set_train_%i.txt\" % count, df.values, fmt='%.5f', delimiter=\",\")\n",
    "\n",
    "    with open(\"datasets/wine_data_set_train_%i.txt\" % count, \"rb\") as f_in, gzip.open(\"datasets/wine_data_set_train_%i.dat.gz\" % count, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "    os.remove(\"datasets/wine_data_set_train_%i.txt\" % count)\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "count = 0\n",
    "for g, df in test.groupby(np.arange(len(test)) // divisor_test):\n",
    "    print(df.shape)\n",
    "\n",
    "    np.savetxt(\"datasets/wine_data_set_test_%i.txt\" % count, df.values, fmt='%.5f', delimiter=\",\")\n",
    "\n",
    "    with open(\"datasets/wine_data_set_test_%i.txt\" % count, \"rb\") as f_in, gzip.open(\"datasets/wine_data_set_test_%i.dat.gz\" % count, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "    os.remove(\"datasets/wine_data_set_test_%i.txt\" % count)\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "small_train = train.drop(train.index[1675:])\n",
    "small_test = test.drop(test.index[825:])\n",
    "\n",
    "np.savetxt(\"datasets/wine_data_set_train_small.txt\", small_train.values, fmt='%.5f', delimiter=\",\")\n",
    "\n",
    "with open(\"datasets/wine_data_set_train_small.txt\", \"rb\") as f_in, gzip.open(\"datasets/wine_data_set_train_small.dat.gz\", \"wb\") as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "os.remove(\"datasets/wine_data_set_train_small.txt\")\n",
    "              \n",
    "np.savetxt(\"datasets/wine_data_set_test_small.txt\", small_train.values, fmt='%.5f', delimiter=\",\")\n",
    "\n",
    "with open(\"datasets/wine_data_set_test_small.txt\", \"rb\") as f_in, gzip.open(\"datasets/wine_data_set_test_small.dat.gz\", \"wb\") as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "os.remove(\"datasets/wine_data_set_test_small.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into 5 chunks each with 20% of the data. Then, we set aside one chunk of 10% to be our small dataset in EMADE.\n",
    "\n",
    "The final step for preprocessing is to compress all the .dat files into .dat.gz files and place them in a folder under the datasets directory of EMADE.\n",
    "\n",
    "There are other datasets already implemented into EMADE in this format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example xml template for the dataset we preprocessed. This template is used to setup parameters for EMADE. You can change the objectives, crossover probability, and mutation probability from here.\n",
    "\n",
    "Make sure to make an xml file and store it in the templates directory when you implement a new dataset into EMADE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\"?>\n",
    "\n",
    "<input>\n",
    "\n",
    "    <datasets>\n",
    "        <dataset>\n",
    "            <name>SmallDataSet</name>\n",
    "            <type>featuredata</type>\n",
    "            <MonteCarlo>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_small.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_small.dat.gz</testFilename>\n",
    "                </trial>\n",
    "            </MonteCarlo>\n",
    "        </dataset>\n",
    "        <dataset>\n",
    "            <name>FullDataSet</name>\n",
    "            <type>featuredata</type>\n",
    "            <MonteCarlo>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_0.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_0.dat.gz</testFilename>\n",
    "                </trial>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_1.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_1.dat.gz</testFilename>\n",
    "                </trial>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_2.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_2.dat.gz</testFilename>\n",
    "                </trial>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_3.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_3.dat.gz</testFilename>\n",
    "                </trial>\n",
    "                <trial>\n",
    "                    <trainFilename>datasets/wine/wine_data_set_train_4.dat.gz</trainFilename>\n",
    "                    <testFilename>datasets/wine/wine_data_set_test_4.dat.gz</testFilename>\n",
    "                </trial>\n",
    "            </MonteCarlo>\n",
    "        </dataset>\n",
    "    </datasets>\n",
    "\n",
    "    <objectives>\n",
    "        <objective>\n",
    "            <name>False Positives</name>\n",
    "            <weight>-1.0</weight>\n",
    "            <achievable>4971.8</achievable>\n",
    "            <goal>0</goal>\n",
    "            <evaluationFunction>false_positive</evaluationFunction>\n",
    "            <lower>0</lower>\n",
    "            <upper>1</upper>\n",
    "        </objective>\n",
    "        <objective>\n",
    "            <name>False Negatives</name>\n",
    "            <weight>-1.0</weight>\n",
    "            <achievable>1541.2</achievable>\n",
    "            <goal>0</goal>\n",
    "            <evaluationFunction>false_negative</evaluationFunction>\n",
    "            <lower>0</lower>\n",
    "            <upper>1</upper>\n",
    "        </objective>\n",
    "        <objective>\n",
    "            <name>F1-Score</name>\n",
    "            <weight>-1.0</weight>\n",
    "            <achievable>0.2</achievable>\n",
    "            <goal>0</goal>\n",
    "            <evaluationFunction>f1_score_min</evaluationFunction>\n",
    "            <lower>0</lower>\n",
    "            <upper>1</upper>\n",
    "        </objective>\n",
    "        <objective>\n",
    "            <name>Num Elements</name>\n",
    "            <weight>-1.0</weight>\n",
    "            <achievable>100.0</achievable>\n",
    "            <goal>0</goal>\n",
    "            <evaluationFunction>num_elements_eval_function</evaluationFunction>\n",
    "            <lower>0</lower>\n",
    "            <upper>1</upper>\n",
    "        </objective>\n",
    "\n",
    "    </objectives>\n",
    "\n",
    "    <evaluation>\n",
    "        <module>evalFunctions</module>\n",
    "        <memoryLimit>30</memoryLimit> <!-- In Percent -->\n",
    "    </evaluation>\n",
    "\n",
    "    <scoopParameters>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>24</workers>\n",
    "        </host>\n",
    "        <!--<host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>\n",
    "        <host>\n",
    "            <name>localhost</name>\n",
    "            <workers>3</workers>\n",
    "        </host>-->\n",
    "    </scoopParameters>\n",
    "\n",
    "    <evolutionParameters>\n",
    "        <initialPopulationSize>512</initialPopulationSize>\n",
    "        <elitePoolSize>512</elitePoolSize>\n",
    "        <launchSize>300</launchSize>\n",
    "        <minQueueSize>200</minQueueSize>\n",
    "\n",
    "        <matings>\n",
    "            <mating>\n",
    "                <name>crossover</name>\n",
    "                <probability>0.50</probability>\n",
    "            </mating>\n",
    "            <mating>\n",
    "                <name>crossoverEphemeral</name>\n",
    "                <probability>0.50</probability>\n",
    "            </mating>\n",
    "            <mating>\n",
    "                <name>headlessChicken</name>\n",
    "                <probability>0.10</probability>\n",
    "            </mating>\n",
    "            <mating>\n",
    "                <name>headlessChickenEphemeral</name>\n",
    "                <probability>0.10</probability>\n",
    "            </mating>\n",
    "        </matings>\n",
    "\n",
    "        <mutations>\n",
    "            <mutation>\n",
    "                <name>insert</name>\n",
    "                <probability>0.05</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>insert modify</name>\n",
    "                <probability>0.10</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>ephemeral</name>\n",
    "                <probability>0.25</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>node replace</name>\n",
    "                <probability>0.05</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>uniform</name>\n",
    "                <probability>0.05</probability>\n",
    "            </mutation>\n",
    "            <mutation>\n",
    "                <name>shrink</name>\n",
    "                <probability>0.05</probability>\n",
    "            </mutation>\n",
    "        </mutations>\n",
    "\n",
    "        <selections>\n",
    "            <selection>\n",
    "                <name>NSGA2</name>\n",
    "            </selection>\n",
    "        </selections>\n",
    "\n",
    "    </evolutionParameters>\n",
    "\n",
    "    <seedFile>\n",
    "\n",
    "    </seedFile>\n",
    "\n",
    "    <genePoolFitness>\n",
    "        <prefix>genePoolFitnessWine</prefix>\n",
    "    </genePoolFitness>\n",
    "    <paretoFitness>\n",
    "        <prefix>paretoFitnessWine</prefix>\n",
    "    </paretoFitness>\n",
    "    <parentsOutput>\n",
    "        <prefix>parentsWine</prefix>\n",
    "    </parentsOutput>\n",
    "\n",
    "\n",
    "\n",
    "    <paretoOutput>\n",
    "        <prefix>paretoFrontWine</prefix>\n",
    "    </paretoOutput>\n",
    "</input>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
